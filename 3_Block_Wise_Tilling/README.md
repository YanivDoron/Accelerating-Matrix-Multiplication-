# üß± Block-Wise Tiling for GPU Parallelism

 
To address naive limitations, we implemented a **block-wise tiled CUDA kernel**, assigning a 16√ó16 thread block to each matrix tile.  
Each thread computes one output element ‚Äî enabling **scalable**, **high-occupancy** GPU utilization.



A better approach is to divide the work across multiple smaller blocks, each with dimensions like 16√ó16 (256 threads). These blocks can be distributed across multiple SMs, enabling concurrent execution and better throughput. This layout also aligns well with warp execution, improves load balancing, and makes the kernel more scalable to larger matrices.

![Performance Plot ‚Äì Naive CUDA](images/block.png)

### ‚úÖ Improvements from Block-Wise Tiling

- üß† **More Blocks ‚Üí More SMs Active**
- üéØ **Better Warp Utilization (256 threads per block)**
- üì∂ **Better Memory Coalescing**
- üîÅ **Scalable Across Matrix Sizes**

---

### üìä Benchmark Results ‚Äì Blocked CUDA

| **Size** | **MKL (ms)** | **Naive (ms)** | **Block (ms)** | **Naive Speedup** | **Block Speedup** |
|----------|--------------|----------------|----------------|-------------------|-------------------|
| 128√ó128  | 0.123        | 0.102          | 0.077          | 1.20√ó             | 1.59√ó             |
| 256√ó256  | 0.534        | 0.099          | 0.084          | 5.40√ó             | 6.37√ó             |
| 512√ó512  | 3.900        | 0.330          | 0.349          | 11.83√ó            | 11.18√ó            |
| 1024√ó1024| 26.826       | 2.944          | 2.354          | 9.11√ó             | 11.40√ó            |
| 2048√ó2048| 201.054      | 18.137         | 15.945         | 11.09√ó            | 12.61√ó            |

![Performance Plot ‚Äì Blocked](images/graph_block.png)

---

### üî• Flame Graph ‚Äì Block-Wise CUDA

Each block computes a matrix tile ‚Üí multiple blocks across SMs = full device utilization.  
Memory latency is better hidden due to concurrency.

![Flame Graph ‚Äì Block](images/flame_block.png)

---

## üìå Summary

| Feature                  | Naive CUDA                | Block-Wise CUDA            |
|--------------------------|---------------------------|----------------------------|
| Threads per block        | 1024                      | 256 (16√ó16)                |
| Number of blocks         | 1                         | (N / 16) √ó (N / 16)        |
| SM Utilization           | ‚ùå Low                    | ‚úÖ High                    |
| Scalability              | ‚ùå Poor                   | ‚úÖ Excellent               |
| Speedup vs MKL (max)     | ~11√ó                      | ~12.6√ó                     |

---

## üìä Running and Profiling the Matrix Multiplication Implementations

This section explains how to benchmark, profile, and analyze all GPU implementations in the project.

---

### üîß Scripts Overview

| Script                   | Description                                                                 |
|--------------------------|-----------------------------------------------------------------------------|
| `Benchmark.py`           | Benchmarks all implementations and prints speedup vs MKL                   |
| `GenerateFlameGraph.py`  | Runs profiler, saves Chrome timeline (log/profile.json)                    |
| `ShowPerformance.py`     | Runs profiler and saves tabular data to `Profile.txt`                      |
| `script_benchmark_<X>.sh`| Bash runner for specific kernels across matrix sizes                        |

---

### üìà `Benchmark.py`

**What it does:**
- Runs all GPU matmul implementations on several matrix sizes.
- Compares performance to MKL and previous kernels.

**Output:**
- `results/times.npy`
- `results/speedups.npy`

**Usage:**
```bash
python Benchmark.py
