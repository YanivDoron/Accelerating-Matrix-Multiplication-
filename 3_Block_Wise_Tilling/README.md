# ğŸ§± Block-Wise Tiling for GPU Parallelism

 
To address naive limitations, we implemented a **block-wise tiled CUDA kernel**, assigning a 16Ã—16 thread block to each matrix tile.  
Each thread computes one output element â€” enabling **scalable**, **high-occupancy** GPU utilization.

![Performance Plot â€“ Naive CUDA](images/block.png)
---

### âš ï¸ Limitation of the Naive Kernel

- Uses 1 block of 1024 threads â†’ engages only 1 SM.
- Low GPU occupancy.
- Not scalable to large matrices.

---

### âœ… Improvements from Block-Wise Tiling

- ğŸ§  **More Blocks â†’ More SMs Active**
- ğŸ¯ **Better Warp Utilization (256 threads per block)**
- ğŸ“¶ **Better Memory Coalescing**
- ğŸ” **Scalable Across Matrix Sizes**

---

### ğŸ“Š Benchmark Results â€“ Blocked CUDA

| **Size** | **MKL (ms)** | **Naive (ms)** | **Block (ms)** | **Naive Speedup** | **Block Speedup** |
|----------|--------------|----------------|----------------|-------------------|-------------------|
| 128Ã—128  | 0.123        | 0.102          | 0.077          | 1.20Ã—             | 1.59Ã—             |
| 256Ã—256  | 0.534        | 0.099          | 0.084          | 5.40Ã—             | 6.37Ã—             |
| 512Ã—512  | 3.900        | 0.330          | 0.349          | 11.83Ã—            | 11.18Ã—            |
| 1024Ã—1024| 26.826       | 2.944          | 2.354          | 9.11Ã—             | 11.40Ã—            |
| 2048Ã—2048| 201.054      | 18.137         | 15.945         | 11.09Ã—            | 12.61Ã—            |

![Performance Plot â€“ Blocked](images/graph_block.png)

---

### ğŸ”¥ Flame Graph â€“ Block-Wise CUDA

Each block computes a matrix tile â†’ multiple blocks across SMs = full device utilization.  
Memory latency is better hidden due to concurrency.

![Flame Graph â€“ Block](images/flame_block.png)

---

## ğŸ“Œ Summary

| Feature                  | Naive CUDA                | Block-Wise CUDA            |
|--------------------------|---------------------------|----------------------------|
| Threads per block        | 1024                      | 256 (16Ã—16)                |
| Number of blocks         | 1                         | (N / 16) Ã— (N / 16)        |
| SM Utilization           | âŒ Low                    | âœ… High                    |
| Scalability              | âŒ Poor                   | âœ… Excellent               |
| Speedup vs MKL (max)     | ~11Ã—                      | ~12.6Ã—                     |

---

## ğŸ“Š Running and Profiling the Matrix Multiplication Implementations

This section explains how to benchmark, profile, and analyze all GPU implementations in the project.

---

### ğŸ”§ Scripts Overview

| Script                   | Description                                                                 |
|--------------------------|-----------------------------------------------------------------------------|
| `Benchmark.py`           | Benchmarks all implementations and prints speedup vs MKL                   |
| `GenerateFlameGraph.py`  | Runs profiler, saves Chrome timeline (log/profile.json)                    |
| `ShowPerformance.py`     | Runs profiler and saves tabular data to `Profile.txt`                      |
| `script_benchmark_<X>.sh`| Bash runner for specific kernels across matrix sizes                        |

---

### ğŸ“ˆ `Benchmark.py`

**What it does:**
- Runs all GPU matmul implementations on several matrix sizes.
- Compares performance to MKL and previous kernels.

**Output:**
- `results/times.npy`
- `results/speedups.npy`

**Usage:**
```bash
python Benchmark.py
