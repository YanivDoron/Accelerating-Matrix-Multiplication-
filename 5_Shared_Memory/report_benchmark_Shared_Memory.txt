--------------------------------------------------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------- Shared Memory Implementation - Matrix Multiplication ------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                             For visual results and detailed benchmarks, see the README.md on GitHub.
                                             For detailed code documentation, refer to:      kernel_example.cu
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------
------------------------------- Overview: ------------------------------------
------------------------------------------------------------------------------

Shared memory in CUDA is a fast, low-latency memory space located on-chip and accessible by all threads in the same block.
Unlike global memory, which resides in DRAM and is shared across the entire GPU, shared memory provides much quicker access,
typically at a latency similar to registers. Because of this, it is a powerful tool for optimizing performance in GPU program.

Each thread block gets its own dedicated shared memory space. Threads in the block cooperatively
load data into shared memory a sub part of matrices A and B  , then use this data to perform calculations.
Synchronization is used to ensure all threads have completed their loads before the computation proceeds.

Once the necessary computation is done, results can be written back to global memory.



---------------------------------------
------------ Libraries used -----------
---------------------------------------
1. PyTorch (CUDA Extension): Used for defining and launching custom CUDA kernels from Python.

2. CUDA Toolkit:Provides the low-level programming interface for GPU execution,
    including thread/block management and memory handling.

3. NumPy: Used for CPU-side matrix initialization and validation .

4. MKL Intel’s Math Kernel Library Used for CPU-side matrix performance comparison .

---------------------------------------
-------Data Structures Employed -------
---------------------------------------

1. Dense 2D Arrays (Row-Major Layout):
   Matrices are stored as standard contiguous 2D arrays in memory.
   MKL expects input matrices in row-major (C-style) layout unless specified otherwise.

2. Float32 or Float64 Tensors:
   Depending on the configuration, matrices are represented using single-precision (float32) or double-precision (float64) elements,
   balancing between performance and numerical accuracy.

3. Shared  Memory:
   Parts of the input matrices are loaded into shared memory, allowing threads within a block to access reused data efficiently. 
   Each thread is responsible for loading specific elements into shared memory and all computation is then performed 
   using these cached tiles. Output tensors remain in global memory.

4. Thread-Level Output Mapping:
   Each CUDA thread computes a single element of the output matrix by iterating over the shared dimension.

5. CUDA Streams:
   Multiple CUDA streams are created to enable concurrent execution of independent matrix tiles. Each stream handles a separate sub-region of the output matrix.

6. Submatrix Slices:
   Matrices A and B are logically partitioned into smaller tiles. These slices are passed to individual kernel launches per stream without duplicating the data.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------
------------------------------- Initial Analysis: ----------------------------
------------------------------------------------------------------------------

The main benefit of shared memory is speed. reading from and writing to shared memory is much faster than accessing global memory.
 This is especially important in compute-heavy operations where the same data may be needed multiple times by different threads.
 Using shared memory allows that data to be reused efficiently without reloading it from global memory, leading to major performance gains.

---------------------------------------
------- Performance Analysis: ---------
---------------------------------------

The benchmark results clearly demonstrate the performance benefits of using shared memory in CUDA matrix multiplication kernels.
By comparing execution times across four GPU implementations—naive block-based,streamed and their shared-memory optimized versions—
we observe consistent and substantial speedups as matrix size increases.

----------------------------------------
--------------Flame Graphs:------------- 
----------------------------------------

Observation 1: Kernel Duration
In the block-based implementation (matmul_naive_block), a single long kernel call performs the entire matrix multiplication.
In contrast, the shared memory implementation launches multiple shorter kernels, each likely computing a small tile of the output matrix.
This indicates that the workload is divided into smaller parallel tasks.

Observation 2: Streaming and Overlap
The shared memory implementation uses multiple CUDA streams (streams 7–27), with each stream running an independent kernel.
This enables overlap between kernel executions, improving concurrency and keeping more streaming multiprocessors (SMs) active.

Observation 3: Idle Time and Synchronization

The block-based implementation includes several cudaStreamSynchronize calls, introducing serialization and idle periods between stages.
In the shared memory implementation, synchronization is minimized, allowing for continuous and efficient kernel execution across streams.

---------------------------------------
-------------Profiling Data:-----------
---------------------------------------
- Shared memory used: 8192 B  
- Blocks per SM: ~60.23529  
- Warps per SM: ~1927.5294  
- Estimated achieved occupancy: 100%

- Grid size: (64, 64, 1)  
- Block size: (16, 16, 1)
-----------------------------------------
---------------Analysis:-----------------
-----------------------------------------
- The use of 16×16 blocks (256 threads per block) allows for a high number of concurrent blocks per SM, improving GPU utilization.
- Achieving 100% occupancy indicates optimal use of available registers and warp slots per SM.
- Moderate shared memory usage (8 KB per block) enables high concurrency while still allowing efficient tile-based reuse.
- The grid of 64×64 blocks ensures full coverage of large matrices and distributes work evenly across all SMs.
- The relatively high reported warps per SM is due to the fact that the flame graph was generated on a GPU different(a regular RTX 2080) 
  from the baseline ( RTX 2080 Ti). Device-specific differences in SM configuration and profiling interpretation can affect the apparent number of warps.

-----------------------------------------
Disclaimer Regarding Shared Memory Usage:
-----------------------------------------
Although the GPU provides up to 48KB of shared memory per block,
we intentionally used a smaller shared memory size in our implementation.
Empirical testing showed that increasing the shared memory allocation beyond a certain point did not improve performance — and in some cases,
even degraded it. This suggests that memory size alone is not the limiting factor,
and optimal performance is achieved with a more conservative shared memory footprint.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------
---------------------------------- Optimizations: ----------------------------
------------------------------------------------------------------------------

The Shared Memory CUDA implementation introduces key optimizations over the Block version to improve GPU resource utilization and scalability.

- Tiling with shared memory reduces redundant global memory accesses and improves data locality.
- Data loaded into shared memory is reused across multiple threads within a block, increasing memory bandwidth efficiency.
- Register tiling inside each thread further reduces shared memory pressure and enhances computational throughput.
- The use of multiple CUDA streams enables concurrent execution of tile-level kernels, improving SM utilization and hiding synchronization overhead.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------
--------------------------- Performance Comparison ---------------------------
------------------------------------------------------------------------------

Comparison: Shared Memory vs. Blocked & Streaming Implementation
---------------------------------------------

Execution Time Comparison (ms)
------------------------------

| Size  | Block (No SM)  | Stream (No SM) | Block (With SM) | Stream (With SM) |
|-------|----------------|----------------|------------------|------------------|
| 128   | 0.10           | 0.12           | 0.09             | 0.11             |
| 256   | 0.10           | 0.13           | 0.11             | 0.12             |
| 512   | 0.37           | 0.39           | 0.28             | 0.28             |
| 1024  | 2.38           | 2.40           | 1.57             | 1.57             |
| 1280  | 4.71           | 4.73           | 2.99             | 2.99             |
| 1408  | 6.20           | 6.22           | 3.96             | 3.97             |
| 1792  | 12.68          | 12.70          | 8.13             | 8.13             |
| 2048  | 13.79          | 15.08          | 9.41             | 9.63             |

Analysis
--------

1. Shared Memory Impact  
   Across all matrix sizes, enabling shared memory significantly reduces execution time for both block-based and streamed versions.  
   The effect becomes more pronounced as the matrix size increases.  
   For example, at size 2048, shared memory reduces the block-based time from 13.79 ms to 9.41 ms (a ~32% improvement).

2. Block vs Stream  
   - Without shared memory, block-based execution slightly outperforms streamed execution at all sizes.  
   - With shared memory, the gap between block and stream narrows, and both approaches achieve nearly identical performance for large matrices.

3. Scalability  
   - The benefit of shared memory increases with matrix size, due to better reuse and reduced global memory traffic.  
   - At small sizes (128–256), shared memory has marginal or no benefit due to limited memory pressure and kernel launch overhead dominating.

4. Bandwidth Efficiency  
   - Shared memory allows for more efficient reuse of data, reducing the number of global memory transactions and improving cache locality.  
   - As matrix size increases, this reuse becomes critical for sustaining throughput and reducing memory bottlenecks.

5. Streamed Version Plateau  
   - For large sizes (1024+), both with and without shared memory, the streamed version shows diminishing returns,  
     indicating that the overhead of managing many small kernels across streams eventually outweighs the concurrency benefits.

-----------------------------------------
Disclaimer Regarding Shared Memory Usage:
-----------------------------------------
Although the GPU provides up to 48KB of shared memory per block,
we intentionally used a smaller shared memory size in our implementation.
Empirical testing showed that increasing the shared memory allocation beyond a certain point did not improve performance — and in some cases,
even degraded it. This suggests that memory size alone is not the limiting factor,
and optimal performance is achieved with a more conservative shared memory footprint.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------
-------------------- Hardware Acceleration Proposal --------------------
------------------------------------------------------------------------

To further accelerate matrix multiplication, we suggest:
-High register utilization through register tiling to reduce memory latency.

-Double Buffering in Shared Memory (Validated but Not Fully Integrated Yet)
 Overlap computation and memory loading to hide latency and improve pipeline efficiency.

-Vectorized Data Loads & Stores (Validated but Not Fully Integrated Yet)
Use vectorized memory instructions to reduce global memory transactions.

These techniques work together to improve GPU occupancy, minimize memory bottlenecks and unlock significantly higher performance on large-scale workloads

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------
-------------------------------- Summary -------------------------------
------------------------------------------------------------------------

The evaluation of four CUDA matrix multiplication kernels shows that shared memory significantly improves performance, especially for large matrices.
Block-based implementations benefit the most, with up to 50% reduction in runtime when shared memory is used.
Register tiling and shared memory together reduce global memory access and increase data reuse within tiles.
Flame graph analysis confirms better SM utilization and reduced idle time under streamed execution.
Kernel profiling shows 100% occupancy with efficient resource use.
Overall, shared memory is critical for scaling performance in GPU-based matrix multiplication.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------